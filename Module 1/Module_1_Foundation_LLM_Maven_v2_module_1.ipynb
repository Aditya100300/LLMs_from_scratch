{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditya100300/LLMs_from_scratch/blob/main/Module%201/Module_1_Foundation_LLM_Maven_v2_module_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring Language Models: Trigram Model & GPT-2 Generation\n",
        "\n",
        "In this notebook, I explore two approaches to language modeling:\n",
        "\n",
        "1. **Trigram Language Model with NLTK**  \n",
        "   - Build a simple count-based trigram model using the Reuters corpus.\n",
        "   - Convert trigram counts into probabilities.\n",
        "   - Query the model to understand word prediction based on two previous words.\n",
        "\n",
        "2. **Text Generation with GPT-2**  \n",
        "   - Use a pre-trained GPT-2 model from the `pytorch-transformers` library.\n",
        "   - Predict the next token for a given input.\n",
        "   - Extend predictions to generate multiple tokens.\n",
        "\n",
        "Each section is broken down into detailed steps with bullet points for clarity.\n"
      ],
      "metadata": {
        "id": "y2N_h0-7B0FB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2I7DweFkH0xG"
      },
      "outputs": [],
      "source": [
        "# Cell: Install NLTK\n",
        "# This command installs the nltk library quietly.\n",
        "# - The \"quiet\" flag reduces the installation output.\n",
        "!pip install nltk --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Trigram Language Model with NLTK\n",
        "\n",
        "In this section, we will:\n",
        "- **Import** required libraries from NLTK and Python's collections.\n",
        "- **Download** the Reuters corpus and tokenizer data.\n",
        "- **Build** a trigram model:\n",
        "  - Iterate over sentences in the corpus.\n",
        "  - Create trigrams with sentence padding.\n",
        "  - Count occurrences of each trigram.\n",
        "- **Convert** the counts into probabilities.\n"
      ],
      "metadata": {
        "id": "IWQeLRWhCD4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "# Download the missing punkt_tab data package.\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# Create a placeholder for the language model\n",
        "# Using nested defaultdict to automatically handle new keys\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Build the trigram model\n",
        "for sentence in reuters.sents():\n",
        "    # Generate trigrams from each sentence\n",
        "    # pad_right and pad_left add None at the beginning and end of the sentence\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        # Increment the count for this trigram\n",
        "        model[(w1, w2)][w3] += 1\n",
        "\n",
        "# Convert frequency counts to probabilities\n",
        "for w1_w2 in model:\n",
        "    # Calculate total count for this bigram\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "\n",
        "    # Convert each count to a probability\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count\n",
        "\n",
        "# At this point, model contains the probabilities of each word (w3)\n",
        "# given the previous two words (w1, w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5snK0qDwH6pP",
        "outputId": "66f994d0-6471-47fa-cf6f-849c0500014d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting Counts to Probabilities\n",
        "\n",
        "Now we convert the frequency counts into probabilities for each trigram.\n",
        "- For each bigram context `(w1, w2)`:\n",
        "  - Sum the counts of all following words `w3`.\n",
        "  - Divide each individual count by the total to get the probability.\n"
      ],
      "metadata": {
        "id": "STmEwY9cp9Ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Querying the Trigram Model\n",
        "\n",
        "Let's query our trigram model:\n",
        "- We choose the bigram `(\"the\", \"price\")` as the context.\n",
        "- We sort the potential following words by their probabilities.\n",
        "- This helps us see which words are most likely to come after \"the price\".\n"
      ],
      "metadata": {
        "id": "o7EiNDXtCh0c"
      }
    },
    {
      "source": [
        "from collections import Counter\n",
        "from nltk import trigrams\n",
        "\n",
        "# ... your existing imports and model training ...\n",
        "\n",
        "# Function to get probabilities given preceding words\n",
        "def get_probabilities(preceding_words):\n",
        "    probabilities = Counter()\n",
        "    for sentence in reuters.sents():\n",
        "        for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "            if (w1, w2) == preceding_words:\n",
        "                probabilities[w3] += 1\n",
        "    total_count = sum(probabilities.values())\n",
        "    if total_count > 0:\n",
        "        for word in probabilities:\n",
        "            probabilities[word] /= total_count\n",
        "    return probabilities\n",
        "\n",
        "# Get probabilities for \"the price\"\n",
        "preceding_words = (\"the\", \"price\")\n",
        "probabilities = get_probabilities(preceding_words)\n",
        "\n",
        "# Sort and print\n",
        "sorted_probabilities = probabilities.most_common()  # Sort by frequency (descending order)\n",
        "print(\"Most probable words following 'the price', in order:\")\n",
        "for word, prob in sorted_probabilities:\n",
        "    print(f\"{word}: {prob}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "M2uuoD6LAHWw",
        "outputId": "025db9d8-72ac-4b4e-d6c6-6162f20abd69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most probable words following 'the price', in order:\n",
            "of: 0.3209302325581395\n",
            "it: 0.05581395348837209\n",
            "to: 0.05581395348837209\n",
            "for: 0.05116279069767442\n",
            ".: 0.023255813953488372\n",
            "at: 0.023255813953488372\n",
            "adjustment: 0.023255813953488372\n",
            "is: 0.018604651162790697\n",
            ",: 0.018604651162790697\n",
            "paid: 0.013953488372093023\n",
            "increases: 0.013953488372093023\n",
            "per: 0.013953488372093023\n",
            "the: 0.013953488372093023\n",
            "will: 0.013953488372093023\n",
            "cut: 0.009302325581395349\n",
            "cuts: 0.009302325581395349\n",
            "(: 0.009302325581395349\n",
            "differentials: 0.009302325581395349\n",
            "has: 0.009302325581395349\n",
            "stayed: 0.009302325581395349\n",
            "was: 0.009302325581395349\n",
            "freeze: 0.009302325581395349\n",
            "increase: 0.009302325581395349\n",
            "would: 0.009302325581395349\n",
            "yesterday: 0.004651162790697674\n",
            "effect: 0.004651162790697674\n",
            "used: 0.004651162790697674\n",
            "climate: 0.004651162790697674\n",
            "reductions: 0.004651162790697674\n",
            "limit: 0.004651162790697674\n",
            "now: 0.004651162790697674\n",
            "moved: 0.004651162790697674\n",
            "adjustments: 0.004651162790697674\n",
            "slumped: 0.004651162790697674\n",
            "move: 0.004651162790697674\n",
            "evolution: 0.004651162790697674\n",
            "went: 0.004651162790697674\n",
            "factor: 0.004651162790697674\n",
            "Royal: 0.004651162790697674\n",
            "again: 0.004651162790697674\n",
            "changes: 0.004651162790697674\n",
            "holds: 0.004651162790697674\n",
            "fall: 0.004651162790697674\n",
            "-: 0.004651162790697674\n",
            "from: 0.004651162790697674\n",
            "base: 0.004651162790697674\n",
            "on: 0.004651162790697674\n",
            "review: 0.004651162790697674\n",
            "while: 0.004651162790697674\n",
            "collapse: 0.004651162790697674\n",
            "being: 0.004651162790697674\n",
            "outlook: 0.004651162790697674\n",
            "rises: 0.004651162790697674\n",
            "drop: 0.004651162790697674\n",
            "guaranteed: 0.004651162790697674\n",
            ",\": 0.004651162790697674\n",
            "structure: 0.004651162790697674\n",
            "and: 0.004651162790697674\n",
            "could: 0.004651162790697674\n",
            "related: 0.004651162790697674\n",
            "hike: 0.004651162790697674\n",
            "we: 0.004651162790697674\n",
            "policy: 0.004651162790697674\n",
            "revision: 0.004651162790697674\n",
            "led: 0.004651162790697674\n",
            "action: 0.004651162790697674\n",
            "zone: 0.004651162790697674\n",
            "slump: 0.004651162790697674\n",
            "had: 0.004651162790697674\n",
            "difference: 0.004651162790697674\n",
            "in: 0.004651162790697674\n",
            "raise: 0.004651162790697674\n",
            "support: 0.004651162790697674\n",
            "gap: 0.004651162790697674\n",
            "projected: 0.004651162790697674\n",
            "approached: 0.004651162790697674\n",
            "instability: 0.004651162790697674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observations from the Trigram Model\n",
        "\n",
        "- The model leverages **simple counting** to build a probability distribution.\n",
        "- It demonstrates how **local context** (the previous two words) can influence the prediction of the next word.\n",
        "- Although basic, this method introduces fundamental concepts of language modeling and probability estimation.\n"
      ],
      "metadata": {
        "id": "ukYr-iHnCo-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Text Generation with GPT-2\n",
        "\n",
        "In this section, we'll generate text using the pre-trained GPT-2 model:\n",
        "- **Install** and import the necessary libraries.\n",
        "- **Load** the GPT-2 tokenizer and model.\n",
        "- **Encode** an input sentence.\n",
        "- **Predict** the next token.\n",
        "- **Extend** the prediction to generate a longer text sequence.\n"
      ],
      "metadata": {
        "id": "VYLh3z2qCzBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-transformers --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-tDCIQWJLmW",
        "outputId": "c9b31bd6-9a26-4da6-c250-27ba068c105e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up GPT-2\n",
        "\n",
        "Let's set up the GPT-2 model:\n",
        "- **Import** torch and GPT-2 classes.\n",
        "- **Load** the tokenizer which converts text to tokens.\n",
        "- **Encode** a given input text into tokens.\n",
        "- **Load** the pre-trained GPT-2 model and set it to evaluation mode.\n",
        "- **Move** the model and input data to GPU if available for faster processing.\n"
      ],
      "metadata": {
        "id": "qPx4_M6iDbVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCdvvrRJKUtA",
        "outputId": "8a5309e2-9d08-46bf-c2eb-a6cc67470bd9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1042301/1042301 [00:05<00:00, 178153.67B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 523943.40B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input text\n",
        "text = \"I am thinking\"\n",
        "print(f\"Input text: {text}\")\n",
        "\n",
        "# Encode the input text\n",
        "indexed_tokens = tokenizer.encode(text)\n",
        "\n",
        "# Convert indexed tokens to a PyTorch tensor\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "model.eval()\n",
        "\n",
        "# Check if CUDA is available and move model and tensors to GPU if possible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokens_tensor = tokens_tensor.to(device)\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6xBHpdMsBHU",
        "outputId": "371d0da7-9855-495c-d66b-197874b7c1b3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text: I am thinking\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 665/665 [00:00<00:00, 2336023.58B/s]\n",
            "100%|██████████| 548118077/548118077 [00:41<00:00, 13070284.37B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting the Next Token with GPT-2\n",
        "\n",
        "Using the input text, we now predict the next token:\n",
        "- **Disable gradients** since we're only doing inference.\n",
        "- **Pass** the input tensor through the model to get predictions.\n",
        "- **Extract** the most likely token from the output.\n",
        "- **Decode** the predicted token back into text and append it to the input.\n"
      ],
      "metadata": {
        "id": "fIxAriAVDpd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict next token\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0]\n",
        "\n",
        "# Get the predicted next sub-word (token)\n",
        "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "predicted_token = tokenizer.decode([predicted_index])\n",
        "\n",
        "# Add the predicted token to the original text\n",
        "predicted_text = text + predicted_token\n",
        "\n",
        "# Print the results\n",
        "print(f\"Predicted next token: '{predicted_token}'\")\n",
        "print(f\"Complete predicted text: '{predicted_text}'\")"
      ],
      "metadata": {
        "id": "a2uBQHYssUCQ",
        "outputId": "6e022e48-42e9-48fb-8607-5b7c50e6a598",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted next token: ' of'\n",
            "Complete predicted text: 'I am thinking of'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extending Text Generation: Predicting Multiple Tokens\n",
        "\n",
        "We can extend the generation process to predict a sequence of tokens:\n",
        "- **Loop** for a defined number of tokens.\n",
        "- **Update** the input with each new token.\n",
        "- **Generate** a longer, coherent piece of text.\n"
      ],
      "metadata": {
        "id": "UYR7DswSDuSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Generate multiple next tokens\n",
        "num_tokens_to_generate = 100\n",
        "generated_text = text\n",
        "\n",
        "for _ in range(num_tokens_to_generate):\n",
        "    # Encode all text generated so far\n",
        "    indexed_tokens = tokenizer.encode(generated_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
        "\n",
        "    # Predict next token\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor)\n",
        "        predictions = outputs[0]\n",
        "\n",
        "    # Get the predicted next token\n",
        "    predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "    predicted_token = tokenizer.decode([predicted_index])\n",
        "\n",
        "    # Add the predicted token to the generated text\n",
        "    generated_text += predicted_token\n",
        "\n",
        "print(f\"\\nGenerated text with {num_tokens_to_generate} additional tokens:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "0VsRa-N-r1pC",
        "outputId": "cc5de74f-edee-4e42-ec68-ce8a664dde51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text with 100 additional tokens:\n",
            "I am thinking of doing a book about the history of the United States. I am thinking of doing a book about the history of the United States. I am thinking of doing a book about the history of the United States. I am thinking of doing a book about the history of the United States. I am thinking of doing a book about the history of the United States. I am thinking of doing a book about the history of the United States. I am thinking of doing a book about the history of the United\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Thoughts & Next Steps\n",
        "\n",
        "- **Trigram Model:**  \n",
        "  - Demonstrates basic language modeling using count-based probabilities.\n",
        "  - Reinforces how local context affects word prediction.\n",
        "- **GPT-2 Generation:**  \n",
        "  - Leverages deep learning for coherent text generation.\n",
        "  - Shows the difference between statistical models and neural language models.\n",
        "\n",
        "### What I Plan to Explore Next:\n",
        "- Experiment with different input texts to see varied GPT-2 outputs.\n",
        "- Adjust generation parameters (e.g., temperature, top-k sampling) for diverse results.\n",
        "- Extend the trigram model with higher n-grams or smoothing techniques.\n",
        "\n",
        "Feel free to modify and expand upon these experiments as I continue learning about language models!\n"
      ],
      "metadata": {
        "id": "FNGVXzY8BzI2"
      }
    }
  ]
}